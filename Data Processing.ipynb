{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importing required modules**\n",
    "* pip install pandas\n",
    "* pip install torch\n",
    "* pip install sklearn\n",
    "* pip install transformers\n",
    "* pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing file**\n",
    "* Change directory in read_json to file location on your pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_json('C:/Users/koari/OneDrive/Desktop/IE4483 Proj/train.json')\n",
    "training_data = pd.DataFrame(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cleaning Training Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Checking that there are no None values in 'Reviews' or 'Sentiment' Column**\n",
    "* Output shows that there are no None values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Checking that there are no duplicate data in the training dataset**\n",
    "* Output shows there are 216 instances of duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing duplicate data from training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=training_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Checking and correcting simple spelling mistakes**\n",
    "* IE : (Reccomend -> Recommend)\n",
    "* **WARNING: TAKES VERY LONG TO RUN (30 mins++)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrected_spelling(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "training_data['reviews']=training_data['reviews'].apply(corrected_spelling)\n",
    "training_data.to_json(path_or_buf='C:/Users/koari/OneDrive/Desktop/IE4483 Proj/cleaned_train.json', orient='records',lines=True) # Save point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Start here if you do not want to re-preprocess data**\n",
    "* Loads saved pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=pd.read_json('C:/Users/koari/OneDrive/Desktop/IE4483 Proj/cleaned_train.json',lines=True)\n",
    "training_data=pd.DataFrame(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Checking for class imbalance**\n",
    "* Training data has 85% positve reviews\n",
    "* Class imbalance - impact can be reduced via class weights in training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data.sentiments.value_counts())\n",
    "print(6139/(1049+6139)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Splitting Training Dataset into a training dataset and testing dataset for binary sentiment classification**\n",
    "* Random Seed = 42\n",
    "* Train = 75%, Test = 25% is used but Train = 80%, Test = 20% is another possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(training_data['reviews'], training_data['sentiments'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing pretrained bert model and tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convert the cleaned data into a readable format for the training process using tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the reviews and convert them into BERT's input format\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert the labels into torch tensors\n",
    "train_labels = torch.tensor(train_labels.tolist())\n",
    "test_labels = torch.tensor(test_labels.tolist())\n",
    "\n",
    "# Create a dataset class to load the data into PyTorch\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training Process & parameters**\n",
    "* **WARNING: THIS TAKES 1-2 HRS ON MY PC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory for model checkpoints\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size for training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10\n",
    ")\n",
    "\n",
    "# Create the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the pre-trained BERT model\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset            # evaluation dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Performing analysis on test set & Calculation of  Accuracy, Precision, Recall, F1-Score for further fine tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Get the predicted labels\n",
    "preds = torch.argmax(torch.from_numpy(predictions.predictions), axis=-1)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score\n",
    "accuracy = accuracy_score(test_labels, preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, preds, average='binary')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Save model and tokenizer**\n",
    "* **DO NOT RUN if you have not trained the model above** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./saved_model')\n",
    "tokenizer.save_pretrained('./saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load model and tokenizer**\n",
    "* **Start here to skip data pre-processing & model training** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./saved_model')\n",
    "model = BertForSequenceClassification.from_pretrained('./saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import test.json for prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = pd.read_json('C:/Users/koari/OneDrive/Desktop/IE4483 Proj/test.json')\n",
    "prediction_data = pd.DataFrame(prediction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Performing prediction and storing to new data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review):\n",
    "    # Tokenize the review\n",
    "    inputs = tokenizer(review, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    \n",
    "    # Perform the prediction\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Return Sentiment Value (0 for negative, 1 for postive)\n",
    "    return torch.argmax(probs).item()\n",
    "\n",
    "# Stores the sentiments in a list\n",
    "prediction_list = []\n",
    "for row in range(len(prediction_data)):\n",
    "    prediction_list.append(predict_sentiment(prediction_data.iloc[row][0]))\n",
    "\n",
    "# Append the sentiment list to the a new dataframe\n",
    "finalized_prediction = prediction_data.assign(sentiment=prediction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Store the finalized prediction in CSV File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new datafram into a json file\n",
    "finalized_prediction.to_csv(path_or_buf='C:/Users/koari/OneDrive/Desktop/IE4483 Proj/output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
